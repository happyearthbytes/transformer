{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m pip install --upgrade pip;\n",
    "!python -m pip install ipywidgets;\n",
    "!python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121;\n",
    "!python -m pip install datasets torchtext;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=0 # use fixed seed for testing\n",
    "preview = slice(None,10,None) # first 10 items\n",
    "example_item = 1337\n",
    "max_length = 512 # trim sequences\n",
    "min_freq = 5 # filter uncommon tokens\n",
    "special_tokens = [\"<unk>\", \"<pad>\"]\n",
    "batch_size = 512\n",
    "lr = 0.001\n",
    "# num_epochs = 20\n",
    "num_epochs = 5\n",
    "heads = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example dataset using imdb classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'text', 'label'],\n",
       "     num_rows: 20000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'text', 'label'],\n",
       "     num_rows: 5000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'text', 'label'],\n",
       "     num_rows: 25000\n",
       " }))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"zapsdcn/imdb\", cache_dir=\"../data/classification/\")\n",
    "train_dataset = dataset['train'] \n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'train_14692',\n",
       " 'text': \"Another movie that relies upon the trite, worn-out cliché of the mad scientist gone madder. The movie centers around a surgeon whose life's ambition is to bring the dead...back to life. I know, I know...you've never heard that one before! Of course, as all of these movies go, the experiment goes very, very wrong and creates a maniacal, bloodthirsty creature. For this promising setup, you'd think that it'd be at least a bit suspenseful. Wrong. Like many movies of this era, the idea is nice, but the execution and the script is mediocre. Not the worst horror movie I've seen (no, Abominator: the Evilmaker 2 still takes the cake)...but not one of the gems, either.\",\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[example_item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['another', 'movie', 'that', 'relies', 'upon', 'the', 'trite', ',', 'worn-out', 'cliché']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(train_dataset[example_item][\"text\"])[preview])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer, max_length):\n",
    "    tokens = tokenizer(example[\"text\"])[:max_length]\n",
    "    return {\"tokens\": tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length})\n",
    "validation_dataset = validation_dataset.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length})\n",
    "test_dataset = test_dataset.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_dataset[\"tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size:  25473\n"
     ]
    }
   ],
   "source": [
    "unk_index = vocab[\"<unk>\"]\n",
    "pad_index = vocab[\"<pad>\"]\n",
    "vocab.set_default_index(unk_index)\n",
    "print(\"Vocab Size: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[165, 19, 15, 4084, 732, 2, 3010, 4, 22908, 1577]\n"
     ]
    }
   ],
   "source": [
    "print(vocab.lookup_indices(train_dataset[example_item][\"tokens\"])[preview])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize(example, vocab):\n",
    "    id = vocab.lookup_indices(example[\"tokens\"])\n",
    "    return {\"id\": id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(numericalize, fn_kwargs={\"vocab\": vocab})\n",
    "validation_dataset = validation_dataset.map(numericalize, fn_kwargs={\"vocab\": vocab})\n",
    "test_dataset = test_dataset.map(numericalize, fn_kwargs={\"vocab\": vocab})\n",
    "\n",
    "train_dataset = train_dataset.with_format(type=\"torch\", columns=[\"id\", \"label\"])\n",
    "validation_dataset = validation_dataset.with_format(type=\"torch\", columns=[\"id\", \"label\"])\n",
    "test_dataset = test_dataset.with_format(type=\"torch\", columns=[\"id\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'text', 'label', 'tokens'],\n",
      "    num_rows: 20000\n",
      "})\n",
      "{'id': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
      " 'label': Value(dtype='int64', id=None),\n",
      " 'text': Value(dtype='string', id=None),\n",
      " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\n",
      "DatasetInfo(description='',\n",
      "            citation='',\n",
      "            homepage='',\n",
      "            license='',\n",
      "            features={'id': Sequence(feature=Value(dtype='int64', id=None),\n",
      "                                     length=-1,\n",
      "                                     id=None),\n",
      "                      'label': Value(dtype='int64', id=None),\n",
      "                      'text': Value(dtype='string', id=None),\n",
      "                      'tokens': Sequence(feature=Value(dtype='string', id=None),\n",
      "                                         length=-1,\n",
      "                                         id=None)},\n",
      "            post_processed=None,\n",
      "            supervised_keys=None,\n",
      "            task_templates=None,\n",
      "            builder_name='json',\n",
      "            dataset_name='imdb',\n",
      "            config_name='default',\n",
      "            version=0.0.0,\n",
      "            splits={'test': SplitInfo(name='test',\n",
      "                                      num_bytes=32389511,\n",
      "                                      num_examples=25000,\n",
      "                                      shard_lengths=None,\n",
      "                                      dataset_name='imdb'),\n",
      "                    'train': SplitInfo(name='train',\n",
      "                                       num_bytes=26488371,\n",
      "                                       num_examples=20000,\n",
      "                                       shard_lengths=None,\n",
      "                                       dataset_name='imdb'),\n",
      "                    'validation': SplitInfo(name='validation',\n",
      "                                            num_bytes=6697012,\n",
      "                                            num_examples=5000,\n",
      "                                            shard_lengths=None,\n",
      "                                            dataset_name='imdb')},\n",
      "            download_checksums={'hf://datasets/zapsdcn/imdb@796c4672c878c2479d9ed570ed1609405aca1d96/imdb_dev.jsonl': {'checksum': None,\n",
      "                                                                                                                       'num_bytes': 6810733},\n",
      "                                'hf://datasets/zapsdcn/imdb@796c4672c878c2479d9ed570ed1609405aca1d96/imdb_test.jsonl': {'checksum': None,\n",
      "                                                                                                                        'num_bytes': 32953418},\n",
      "                                'hf://datasets/zapsdcn/imdb@796c4672c878c2479d9ed570ed1609405aca1d96/imdb_train.jsonl': {'checksum': None,\n",
      "                                                                                                                         'num_bytes': 26939140}},\n",
      "            download_size=66703291,\n",
      "            post_processing_size=None,\n",
      "            dataset_size=65574894,\n",
      "            size_in_bytes=132278185)\n"
     ]
    }
   ],
   "source": [
    "pprint(train_dataset)\n",
    "pprint(train_dataset.features)\n",
    "pprint(train_dataset.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_ids = [i[\"id\"] for i in batch]\n",
    "        batch_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            batch_ids, padding_value=pad_index, batch_first=True\n",
    "        )\n",
    "        batch_label = [i[\"label\"] for i in batch]\n",
    "        batch_label = torch.stack(batch_label)\n",
    "        batch = {\"id\": batch_ids, \"label\": batch_label}\n",
    "        return batch\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = get_data_loader(train_dataset, batch_size, pad_index, shuffle=True)\n",
    "validation_data_loader = get_data_loader(validation_dataset, batch_size, pad_index)\n",
    "test_data_loader = get_data_loader(test_dataset, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, emb, heads):\n",
    "        super().__init__()\n",
    "        assert emb % heads == 0\n",
    "        self.emb, self.heads = emb, heads\n",
    "        self.to_queries = torch.nn.Linear(emb, emb)\n",
    "        self.to_keys = torch.nn.Linear(emb, emb)\n",
    "        self.to_values = torch.nn.Linear(emb, emb)\n",
    "        self.unify = torch.nn.Linear(emb, emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, emb = x.shape\n",
    "        h = self.heads\n",
    "        queries = self.to_queries(x)\n",
    "        keys = self.to_keys(x)\n",
    "        values = self.to_values(x)\n",
    "        queries = queries.view(b, t, h, emb//h)\n",
    "        keys = keys.view(b, t, h, emb//h)\n",
    "        values = values.view(b, t, h, emb//h)\n",
    "        queries = queries.transpose(1, 2).reshape(b*h, t, emb//h)\n",
    "        keys = keys.transpose(1, 2).reshape(b*h, t, emb//h)\n",
    "        values = values.transpose(1, 2).reshape(b*h, t, emb//h)\n",
    "        W = torch.bmm(queries, keys.transpose(1,2))\n",
    "        W = W / (emb**(1/2))\n",
    "        W = F.softmax(W, dim=2)\n",
    "        y = torch.bmm(W, values).view(b, h, t, emb//h)\n",
    "        y = y.transpose(1, 2).reshape(b, t, emb)\n",
    "        return self.unify(y), W\n",
    "\n",
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, emb, heads):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttention(emb, heads)\n",
    "        self.norm1 = torch.nn.LayerNorm(emb)\n",
    "        self.norm2 = torch.nn.LayerNorm(emb)\n",
    "        self.fcn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb, 4*emb),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(4*emb, emb)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attented, W = self.attention(x)\n",
    "        x = self.norm1(attented + x)\n",
    "        ff = self.fcn(x)\n",
    "        return self.norm2(ff + x), W\n",
    "\n",
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, emb, heads, max_seq_length, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = torch.nn.Embedding(embedding_dim=emb, num_embeddings=vocab_size)\n",
    "        self.pos_embedding = torch.nn.Embedding(embedding_dim=emb, num_embeddings=max_seq_length)\n",
    "        self.tblock = TransformerBlock(emb=emb, heads=heads)\n",
    "        self.toprobs = torch.nn.Linear(emb, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tokens = self.token_embedding(x)\n",
    "        b, t, e = tokens.shape\n",
    "        positions = self.pos_embedding(torch.arange(t, device=\"cuda\"))[None, :, :].expand(b, t, e)\n",
    "        x = tokens + positions\n",
    "        x, W = self.tblock(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.toprobs(x)\n",
    "        return F.log_softmax(x, dim=1), W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(emb=32, heads=heads, max_seq_length=max_length, vocab_size=len(vocab)).to(\"cuda\")\n",
    "opt = torch.optim.Adam(lr=lr, params=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0; Loss: 0.6780735850334167; Validation Accuracy: 0.5002\n",
      "Epoch:1; Loss: 0.6769123673439026; Validation Accuracy: 0.5234\n",
      "Epoch:2; Loss: 0.5761014223098755; Validation Accuracy: 0.643\n",
      "Epoch:3; Loss: 0.6505365371704102; Validation Accuracy: 0.7048\n",
      "Epoch:4; Loss: 0.5213914513587952; Validation Accuracy: 0.7408\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_data_loader:\n",
    "        opt.zero_grad()\n",
    "        input = batch[\"id\"].to(\"cuda\")\n",
    "        output = batch[\"label\"].to(\"cuda\")\n",
    "        preds, _ = model(input)\n",
    "        loss = F.nll_loss(preds, output)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        with torch.no_grad():\n",
    "            tot, cor= 0.0, 0.0\n",
    "            for batch in validation_data_loader:\n",
    "                input = batch[\"id\"].to(\"cuda\")\n",
    "                output = batch[\"label\"].to(\"cuda\")\n",
    "                if input.shape[1] > max_length:\n",
    "                    input = input[:, :max_length]\n",
    "                preds, _ = model(input)\n",
    "                preds = preds.argmax(dim=1)\n",
    "                tot += float(input.size(0))\n",
    "                cor += float((output == preds).sum().item())\n",
    "            acc = cor / tot\n",
    "            accs.append(acc)\n",
    "    print(\"Epoch:{}; Loss: {}; Validation Accuracy: {}\".format(epoch, loss.item(), acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"trained_models/clasify_{}heads.pt\".format(heads))\n",
    "np.save(\"trained_models/acc.npy\", accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accs = np.load(\"trained_models/acc.npy\")\n",
    "plt.style.use('default')\n",
    "plt.plot(np.linspace(0, 20, len(accs)), accs, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0.45, 1)\n",
    "plt.grid()\n",
    "plt.title(\"Transformer Neural Network\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
